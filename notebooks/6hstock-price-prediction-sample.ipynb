{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Streamlined Advanced Stock Prediction\n",
    "# Focused on TabNet, LightGBM, and Neural Networks\n",
    "\n",
    "# Install core packages\n",
    "# !pip install -q pytorch-tabnet\n",
    "# !pip install -q lightgbm\n",
    "# !pip install -q ta\n",
    "# !pip install -q optuna\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Core ML packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "import optuna\n",
    "import lightgbm as lgb\n",
    "import ta\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "from quant101.core_2.config import sppc_dir\n",
    "\n",
    "# Set seeds\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"Packages loaded successfully!\")\n",
    "\n",
    "# Enhanced LSTM model\n",
    "class EnhancedLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=128, num_layers=3, dropout=0.2):\n",
    "        super(EnhancedLSTM, self).__init__()\n",
    "        \n",
    "        # Multi-layer LSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size, \n",
    "            hidden_size, \n",
    "            num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        # Attention layer\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "        \n",
    "        # Output layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, 10)  # 10 days prediction\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # LSTM forward\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        \n",
    "        # Attention mechanism\n",
    "        attention_weights = F.softmax(self.attention(lstm_out), dim=1)\n",
    "        context = torch.sum(attention_weights * lstm_out, dim=1)\n",
    "        \n",
    "        # Output\n",
    "        output = self.fc(context)\n",
    "        return output\n",
    "\n",
    "# Custom dataset\n",
    "class StockDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = torch.FloatTensor(y)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# Main predictor class\n",
    "class StreamlinedPredictor:\n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        self.scalers = {}\n",
    "        self.feature_names = None\n",
    "        \n",
    "    def load_data(self, data_path):\n",
    "        \"\"\"Load and merge all data\"\"\"\n",
    "        print(\"Loading data...\")\n",
    "        \n",
    "        # Load stock data\n",
    "        dfs = []\n",
    "        for i in range(1, 6):\n",
    "            df = pd.read_csv(f\"{data_path}/test/test_{i}.csv\")\n",
    "            df.columns = [col if col == 'Date' else f\"{col.lower()}_{i}\" for col in df.columns]\n",
    "            dfs.append(df)\n",
    "        \n",
    "        # Merge all stocks\n",
    "        merged = dfs[0]\n",
    "        for df in dfs[1:]:\n",
    "            merged = pd.merge(merged, df, on='Date', how='outer')\n",
    "        \n",
    "        # Load indices\n",
    "        for idx_name, file_name in [('dj', 'Dow_Jones.csv'), ('nasdaq', 'NASDAQ.csv'), ('sp500', 'SP500.csv')]:\n",
    "            idx_df = pd.read_csv(f\"{data_path}/train/indices/{file_name}\")\n",
    "            idx_df = idx_df.rename(columns={'Returns': f'returns_{idx_name}'})\n",
    "            merged = pd.merge(merged, idx_df[['Date', f'returns_{idx_name}']], on='Date', how='left')\n",
    "        \n",
    "        merged['Date'] = pd.to_datetime(merged['Date'])\n",
    "        merged = merged.sort_values('Date').reset_index(drop=True)\n",
    "        \n",
    "        print(f\"Data shape: {merged.shape}\")\n",
    "        return merged\n",
    "    \n",
    "    def engineer_features(self, df):\n",
    "        \"\"\"Create all features\"\"\"\n",
    "        print(\"Engineering features...\")\n",
    "        \n",
    "        # Basic features for each stock\n",
    "        for i in range(1, 6):\n",
    "            close_col = f'close_{i}'\n",
    "            volume_col = f'volume_{i}'\n",
    "            returns_col = f'returns_{i}'\n",
    "            \n",
    "            if close_col in df.columns:\n",
    "                # Fill NaN\n",
    "                df[close_col] = df[close_col].fillna(method='ffill').fillna(method='bfill')\n",
    "                \n",
    "                # Price features\n",
    "                df[f'high_low_ratio_{i}'] = df[close_col].rolling(2).max() / df[close_col].rolling(2).min()\n",
    "                df[f'price_position_{i}'] = (df[close_col] - df[close_col].rolling(20).min()) / (\n",
    "                    df[close_col].rolling(20).max() - df[close_col].rolling(20).min()\n",
    "                )\n",
    "                \n",
    "                # Moving averages\n",
    "                for window in [5, 10, 20, 50]:\n",
    "                    df[f'sma_{window}_{i}'] = ta.trend.sma_indicator(df[close_col], window=window)\n",
    "                    df[f'price_to_sma_{window}_{i}'] = df[close_col] / df[f'sma_{window}_{i}']\n",
    "                \n",
    "                # EMA\n",
    "                df[f'ema_12_{i}'] = ta.trend.ema_indicator(df[close_col], window=12)\n",
    "                df[f'ema_26_{i}'] = ta.trend.ema_indicator(df[close_col], window=26)\n",
    "                \n",
    "                # MACD\n",
    "                macd = ta.trend.MACD(df[close_col])\n",
    "                df[f'macd_{i}'] = macd.macd()\n",
    "                df[f'macd_signal_{i}'] = macd.macd_signal()\n",
    "                df[f'macd_diff_{i}'] = macd.macd_diff()\n",
    "                \n",
    "                # RSI\n",
    "                df[f'rsi_{i}'] = ta.momentum.RSIIndicator(df[close_col], window=14).rsi()\n",
    "                \n",
    "                # Bollinger Bands\n",
    "                bb = ta.volatility.BollingerBands(df[close_col], window=20)\n",
    "                df[f'bb_high_{i}'] = bb.bollinger_hband()\n",
    "                df[f'bb_low_{i}'] = bb.bollinger_lband()\n",
    "                df[f'bb_pct_{i}'] = bb.bollinger_pband()\n",
    "                \n",
    "                # ATR\n",
    "                high = df[close_col].rolling(2).max()\n",
    "                low = df[close_col].rolling(2).min()\n",
    "                df[f'atr_{i}'] = ta.volatility.AverageTrueRange(high, low, df[close_col], window=14).average_true_range()\n",
    "                \n",
    "                # Volume features\n",
    "                if volume_col in df.columns:\n",
    "                    df[volume_col] = df[volume_col].fillna(method='ffill').fillna(0)\n",
    "                    df[f'volume_ratio_{i}'] = df[volume_col] / df[volume_col].rolling(20).mean()\n",
    "                    df[f'volume_trend_{i}'] = df[volume_col].rolling(5).mean() / df[volume_col].rolling(20).mean()\n",
    "                \n",
    "                # Returns features\n",
    "                if returns_col in df.columns:\n",
    "                    # Volatility\n",
    "                    for window in [5, 10, 20]:\n",
    "                        df[f'volatility_{window}_{i}'] = df[returns_col].rolling(window).std()\n",
    "                        df[f'skew_{window}_{i}'] = df[returns_col].rolling(window).skew()\n",
    "                        df[f'kurtosis_{window}_{i}'] = df[returns_col].rolling(window).kurt()\n",
    "                    \n",
    "                    # Momentum\n",
    "                    df[f'momentum_5_{i}'] = df[close_col].pct_change(5)\n",
    "                    df[f'momentum_20_{i}'] = df[close_col].pct_change(20)\n",
    "        \n",
    "        # Cross-stock features\n",
    "        returns_cols = [f'returns_{i}' for i in range(1, 6) if f'returns_{i}' in df.columns]\n",
    "        if len(returns_cols) > 1:\n",
    "            # Correlation features\n",
    "            for window in [20, 60]:\n",
    "                corr_matrix = df[returns_cols].rolling(window).corr()\n",
    "                df[f'avg_correlation_{window}'] = corr_matrix.mean().mean()\n",
    "            \n",
    "            # Dispersion\n",
    "            df['returns_dispersion'] = df[returns_cols].std(axis=1)\n",
    "            df['returns_mean'] = df[returns_cols].mean(axis=1)\n",
    "        \n",
    "        # Market features\n",
    "        market_cols = ['returns_dj', 'returns_nasdaq', 'returns_sp500']\n",
    "        market_cols = [col for col in market_cols if col in df.columns]\n",
    "        \n",
    "        if market_cols:\n",
    "            df['market_returns'] = df[market_cols].mean(axis=1)\n",
    "            df['market_volatility'] = df[market_cols].std(axis=1)\n",
    "            \n",
    "            # Beta to market\n",
    "            for i in range(1, 6):\n",
    "                if f'returns_{i}' in df.columns:\n",
    "                    for market in ['nasdaq', 'sp500']:\n",
    "                        if f'returns_{market}' in df.columns:\n",
    "                            rolling_cov = df[[f'returns_{i}', f'returns_{market}']].rolling(20).cov()\n",
    "                            cov = rolling_cov.xs(f'returns_{market}', level=1)[f'returns_{i}']\n",
    "                            var = df[f'returns_{market}'].rolling(20).var()\n",
    "                            df[f'beta_{i}_{market}'] = cov / var\n",
    "        \n",
    "        # Time features\n",
    "        df['day_of_week'] = df['Date'].dt.dayofweek\n",
    "        df['month'] = df['Date'].dt.month\n",
    "        df['quarter'] = df['Date'].dt.quarter\n",
    "        df['day_of_month'] = df['Date'].dt.day\n",
    "        df['week_of_year'] = df['Date'].dt.isocalendar().week\n",
    "        \n",
    "        # Cyclical encoding\n",
    "        df['day_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
    "        df['day_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
    "        df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "        df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "        \n",
    "        # Lag features\n",
    "        for i in range(1, 6):\n",
    "            if f'returns_{i}' in df.columns:\n",
    "                for lag in [1, 2, 3, 5, 10, 20]:\n",
    "                    df[f'returns_{i}_lag_{lag}'] = df[f'returns_{i}'].shift(lag)\n",
    "                    \n",
    "                # Cumulative returns\n",
    "                df[f'cum_returns_5_{i}'] = df[f'returns_{i}'].rolling(5).sum()\n",
    "                df[f'cum_returns_20_{i}'] = df[f'returns_{i}'].rolling(20).sum()\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def prepare_data(self, df, target_stock, seq_length=60):\n",
    "        \"\"\"Prepare sequences for a specific stock\"\"\"\n",
    "        target_col = f'returns_{target_stock}'\n",
    "        feature_cols = [col for col in df.columns if col not in ['Date'] and \n",
    "                       not col.startswith('returns_') or col == target_col]\n",
    "        \n",
    "        # Remove columns with too many NaN\n",
    "        valid_cols = []\n",
    "        for col in feature_cols:\n",
    "            if df[col].isna().sum() / len(df) < 0.5:\n",
    "                valid_cols.append(col)\n",
    "        \n",
    "        feature_cols = valid_cols\n",
    "        self.feature_names = feature_cols\n",
    "        \n",
    "        # Fill NaN\n",
    "        df_clean = df[feature_cols].fillna(method='ffill').fillna(0)\n",
    "        \n",
    "        # Create sequences\n",
    "        X, y = [], []\n",
    "        \n",
    "        for i in range(seq_length, len(df_clean) - 10):\n",
    "            X.append(df_clean.iloc[i-seq_length:i].values)\n",
    "            \n",
    "            # Next 10 days returns\n",
    "            future_returns = df[target_col].iloc[i:i+10].values\n",
    "            y.append(future_returns)\n",
    "        \n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        \n",
    "        # Remove samples with NaN in target\n",
    "        valid_idx = ~np.isnan(y).any(axis=1)\n",
    "        X = X[valid_idx]\n",
    "        y = y[valid_idx]\n",
    "        \n",
    "        return X, y, feature_cols\n",
    "    \n",
    "    def train_tabnet(self, X_train, y_train, X_val, y_val):\n",
    "        \"\"\"Train TabNet model\"\"\"\n",
    "        print(\"Training TabNet...\")\n",
    "        \n",
    "        # Flatten sequences\n",
    "        X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "        X_val_flat = X_val.reshape(X_val.shape[0], -1)\n",
    "        \n",
    "        # Use mean of future returns as target\n",
    "        y_train_mean = y_train.mean(axis=1).reshape(-1, 1)\n",
    "        y_val_mean = y_val.mean(axis=1).reshape(-1, 1)\n",
    "        \n",
    "        # TabNet model\n",
    "        model = TabNetRegressor(\n",
    "            n_d=32,\n",
    "            n_a=32,\n",
    "            n_steps=5,\n",
    "            gamma=1.5,\n",
    "            lambda_sparse=1e-4,\n",
    "            optimizer_fn=torch.optim.Adam,\n",
    "            optimizer_params=dict(lr=2e-2),\n",
    "            scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "            scheduler_params=dict(mode='min', patience=5, factor=0.5),\n",
    "            mask_type='entmax',\n",
    "            verbose=1,\n",
    "            device_name='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        )\n",
    "        \n",
    "        model.fit(\n",
    "            X_train_flat, y_train_mean,\n",
    "            eval_set=[(X_val_flat, y_val_mean)],\n",
    "            max_epochs=100,\n",
    "            patience=20,\n",
    "            batch_size=256,\n",
    "            virtual_batch_size=128\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def train_lightgbm(self, X_train, y_train, X_val, y_val):\n",
    "        \"\"\"Train LightGBM model\"\"\"\n",
    "        print(\"Training LightGBM...\")\n",
    "        \n",
    "        # Flatten sequences\n",
    "        X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "        X_val_flat = X_val.reshape(X_val.shape[0], -1)\n",
    "        \n",
    "        # Train separate model for each prediction day\n",
    "        models = []\n",
    "        \n",
    "        for day in range(10):\n",
    "            print(f\"Training for day {day+1}...\")\n",
    "            \n",
    "            lgb_model = lgb.LGBMRegressor(\n",
    "                n_estimators=2000,\n",
    "                learning_rate=0.01,\n",
    "                num_leaves=31,\n",
    "                feature_fraction=0.8,\n",
    "                bagging_fraction=0.8,\n",
    "                bagging_freq=5,\n",
    "                min_child_samples=20,\n",
    "                reg_alpha=0.1,\n",
    "                reg_lambda=0.1,\n",
    "                verbose=-1,\n",
    "                random_state=42 + day\n",
    "            )\n",
    "            \n",
    "            lgb_model.fit(\n",
    "                X_train_flat, y_train[:, day],\n",
    "                eval_set=[(X_val_flat, y_val[:, day])],\n",
    "                callbacks=[lgb.early_stopping(100), lgb.log_evaluation(0)]\n",
    "            )\n",
    "            \n",
    "            models.append(lgb_model)\n",
    "        \n",
    "        return models\n",
    "    \n",
    "    def train_lstm(self, X_train, y_train, X_val, y_val):\n",
    "        \"\"\"Train LSTM model\"\"\"\n",
    "        print(\"Training LSTM...\")\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset = StockDataset(X_train, y_train)\n",
    "        val_dataset = StockDataset(X_val, y_val)\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "        \n",
    "        # Model\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        model = EnhancedLSTM(input_size=X_train.shape[2]).to(device)\n",
    "        \n",
    "        # Training setup\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        best_model = None\n",
    "        patience_counter = 0\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(100):\n",
    "            # Train\n",
    "            model.train()\n",
    "            train_loss = 0\n",
    "            for X_batch, y_batch in train_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "            \n",
    "            # Validate\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for X_batch, y_batch in val_loader:\n",
    "                    X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                    outputs = model(X_batch)\n",
    "                    loss = criterion(outputs, y_batch)\n",
    "                    val_loss += loss.item()\n",
    "            \n",
    "            val_loss /= len(val_loader)\n",
    "            scheduler.step(val_loss)\n",
    "            \n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_model = model.state_dict().copy()\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            if patience_counter >= 15:\n",
    "                print(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Epoch {epoch}, Val Loss: {val_loss:.6f}\")\n",
    "        \n",
    "        # Load best model\n",
    "        model.load_state_dict(best_model)\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def predict_ensemble(self, models, X_test, scaler):\n",
    "        \"\"\"Make ensemble predictions\"\"\"\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # Scale test data\n",
    "        X_test_scaled = scaler.transform(X_test.reshape(-1, X_test.shape[-1]))\n",
    "        X_test_scaled = X_test_scaled.reshape(X_test.shape)\n",
    "        \n",
    "        predictions = []\n",
    "        \n",
    "        # TabNet prediction\n",
    "        if 'tabnet' in models:\n",
    "            X_flat = X_test_scaled.reshape(X_test_scaled.shape[0], -1)\n",
    "            tabnet_pred = models['tabnet'].predict(X_flat)\n",
    "            # Expand to 10 days\n",
    "            tabnet_pred_full = np.repeat(tabnet_pred, 10, axis=1).reshape(-1, 10)\n",
    "            predictions.append(tabnet_pred_full)\n",
    "        \n",
    "        # LightGBM predictions\n",
    "        if 'lightgbm' in models:\n",
    "            X_flat = X_test_scaled.reshape(X_test_scaled.shape[0], -1)\n",
    "            lgb_preds = []\n",
    "            for day_model in models['lightgbm']:\n",
    "                day_pred = day_model.predict(X_flat)\n",
    "                lgb_preds.append(day_pred)\n",
    "            lgb_pred_full = np.column_stack(lgb_preds)\n",
    "            predictions.append(lgb_pred_full)\n",
    "        \n",
    "        # LSTM prediction\n",
    "        if 'lstm' in models:\n",
    "            models['lstm'].eval()\n",
    "            with torch.no_grad():\n",
    "                X_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "                lstm_pred = models['lstm'](X_tensor).cpu().numpy()\n",
    "            predictions.append(lstm_pred)\n",
    "        \n",
    "        # Ensemble (weighted average)\n",
    "        if predictions:\n",
    "            # Weights based on model performance (you can adjust these)\n",
    "            weights = [0.3, 0.4, 0.3][:len(predictions)]\n",
    "            weights = np.array(weights) / np.sum(weights)\n",
    "            \n",
    "            ensemble_pred = np.zeros_like(predictions[0])\n",
    "            for i, pred in enumerate(predictions):\n",
    "                ensemble_pred += weights[i] * pred\n",
    "            \n",
    "            return ensemble_pred\n",
    "        else:\n",
    "            return np.zeros((X_test.shape[0], 10))\n",
    "    \n",
    "    def run_pipeline(self, data_path):\n",
    "        \"\"\"Run complete pipeline\"\"\"\n",
    "        print(\"=\"*50)\n",
    "        print(\"Starting Streamlined Advanced Stock Prediction\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # Load data\n",
    "        df = self.load_data(data_path)\n",
    "        \n",
    "        # Engineer features\n",
    "        df = self.engineer_features(df)\n",
    "        \n",
    "        # Prepare submission\n",
    "        sample_submission = pd.read_csv(f'{data_path}/sample_submission.csv')\n",
    "        submission_df = pd.DataFrame()\n",
    "        submission_df['Date'] = sample_submission['Date'][:10]\n",
    "        \n",
    "        # Train model for each stock\n",
    "        for stock_num in range(1, 6):\n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(f\"Processing Stock {stock_num}\")\n",
    "            print(f\"{'='*50}\")\n",
    "            \n",
    "            # Prepare data\n",
    "            X, y, feature_names = self.prepare_data(df, stock_num)\n",
    "            \n",
    "            if len(X) == 0:\n",
    "                print(f\"No valid data for stock {stock_num}, using zeros\")\n",
    "                submission_df[f'return_{stock_num}'] = np.zeros(10)\n",
    "                continue\n",
    "            \n",
    "            print(f\"Data shape: X={X.shape}, y={y.shape}\")\n",
    "            \n",
    "            # Split data\n",
    "            split_idx = int(0.8 * len(X))\n",
    "            X_train, X_val = X[:split_idx], X[split_idx:]\n",
    "            y_train, y_val = y[:split_idx], y[split_idx:]\n",
    "            \n",
    "            # Scale features\n",
    "            scaler = RobustScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1]))\n",
    "            X_train_scaled = X_train_scaled.reshape(X_train.shape)\n",
    "            \n",
    "            X_val_scaled = scaler.transform(X_val.reshape(-1, X_val.shape[-1]))\n",
    "            X_val_scaled = X_val_scaled.reshape(X_val.shape)\n",
    "            \n",
    "            # Train models\n",
    "            stock_models = {}\n",
    "            \n",
    "            try:\n",
    "                # TabNet\n",
    "                tabnet_model = self.train_tabnet(X_train_scaled, y_train, X_val_scaled, y_val)\n",
    "                stock_models['tabnet'] = tabnet_model\n",
    "            except Exception as e:\n",
    "                print(f\"TabNet training failed: {e}\")\n",
    "            \n",
    "            try:\n",
    "                # LightGBM\n",
    "                lgb_models = self.train_lightgbm(X_train_scaled, y_train, X_val_scaled, y_val)\n",
    "                stock_models['lightgbm'] = lgb_models\n",
    "            except Exception as e:\n",
    "                print(f\"LightGBM training failed: {e}\")\n",
    "            \n",
    "            try:\n",
    "                # LSTM\n",
    "                lstm_model = self.train_lstm(X_train_scaled, y_train, X_val_scaled, y_val)\n",
    "                stock_models['lstm'] = lstm_model\n",
    "            except Exception as e:\n",
    "                print(f\"LSTM training failed: {e}\")\n",
    "            \n",
    "            # Store models\n",
    "            self.models[f'stock_{stock_num}'] = stock_models\n",
    "            self.scalers[f'stock_{stock_num}'] = scaler\n",
    "            \n",
    "            # Make predictions\n",
    "            X_test = X[-1:] if len(X) > 0 else X_val[-1:]\n",
    "            predictions = self.predict_ensemble(stock_models, X_test, scaler)\n",
    "            \n",
    "            # Add to submission\n",
    "            submission_df[f'return_{stock_num}'] = predictions[0]\n",
    "            \n",
    "            print(f\"Stock {stock_num} predictions: {predictions[0]}\")\n",
    "        \n",
    "        # Save submission\n",
    "        submission_df.to_csv('streamlined_submission.csv', index=False)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"Prediction completed!\")\n",
    "        print(\"=\"*50)\n",
    "        print(\"\\nSubmission preview:\")\n",
    "        print(submission_df)\n",
    "        \n",
    "        # Plot predictions\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        for i in range(1, 6):\n",
    "            plt.subplot(3, 2, i)\n",
    "            if f'return_{i}' in submission_df.columns:\n",
    "                plt.plot(range(10), submission_df[f'return_{i}'].values, 'o-', label=f'Stock {i}')\n",
    "                plt.xlabel('Days Ahead')\n",
    "                plt.ylabel('Predicted Return')\n",
    "                plt.title(f'Stock {i} - 10 Day Predictions')\n",
    "                plt.grid(True, alpha=0.3)\n",
    "                plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('predictions_plot.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        return submission_df\n",
    "\n",
    "# Run the pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    # Set your data path\n",
    "    DATA_PATH = sppc_dir\n",
    "    \n",
    "    predictor = StreamlinedPredictor()\n",
    "    submission = predictor.run_pipeline(DATA_PATH)\n",
    "    \n",
    "    print(\"\\nAll done! Check 'streamlined_submission.csv' for results.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 11981115,
     "sourceId": 99938,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
